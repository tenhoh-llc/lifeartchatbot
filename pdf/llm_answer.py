"""
LLMï¼ˆOpenAIï¼‰ã‚’ä½¿ç”¨ã—ãŸå›ç­”ç”Ÿæˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
æ¤œç´¢çµæœã‚’åŸºã«LLMã§è‡ªç„¶ãªå›ç­”ã‚’ç”Ÿæˆ
"""
from typing import List, Optional, Dict, Any
from dataclasses import dataclass
import os
from loguru import logger
import openai
from openai import OpenAI
from core.config import AppConfig


@dataclass
class LLMConfig:
    """LLMè¨­å®š"""
    provider: str
    api_key: str
    model: str = "gpt-4o-mini"
    temperature: float = 0.3
    max_tokens: int = 1000


class LLMAnswerGenerator:
    """
    LLMã‚’ä½¿ç”¨ã—ã¦å›ç­”ã‚’ç”Ÿæˆã™ã‚‹ã‚¯ãƒ©ã‚¹
    """

    def __init__(self, config: Optional[LLMConfig] = None):
        """
        åˆæœŸåŒ–

        Args:
            config: LLMè¨­å®š
        """
        if config is None:
            # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿
            app_config = AppConfig.load()
            if app_config.llm_provider == "none":
                self.enabled = False
                self.client = None
                logger.warning("LLM is disabled (provider=none)")
                return

            if app_config.llm_provider != "openai":
                raise ValueError(f"Currently only OpenAI is supported, got: {app_config.llm_provider}")

            config = LLMConfig(
                provider=app_config.llm_provider,
                api_key=app_config.llm_api_key or ""
            )

        self.config = config
        self.enabled = True

        # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
        if config.provider == "openai":
            self.client = OpenAI(api_key=config.api_key)
        else:
            raise ValueError(f"Unsupported provider: {config.provider}")

        logger.info(f"LLM Answer Generator initialized with {config.provider}")

    def generate_answer(
        self,
        query: str,
        search_results: List[Dict[str, Any]],
        context: Optional[str] = None
    ) -> str:
        """
        LLMã‚’ä½¿ç”¨ã—ã¦å›ç­”ã‚’ç”Ÿæˆ

        Args:
            query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¯ã‚¨ãƒª
            search_results: æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
            context: å‰ã®ä¼šè©±ã®æ–‡è„ˆ

        Returns:
            ç”Ÿæˆã•ã‚ŒãŸå›ç­”
        """
        if not self.enabled:
            # LLMãŒç„¡åŠ¹ã®å ´åˆã¯ç°¡æ˜“çš„ãªå›ç­”ã‚’è¿”ã™
            return self._generate_simple_answer(query, search_results)

        if not search_results:
            return "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚è©²å½“ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚ˆã‚Šå…·ä½“çš„ãªè³ªå•ã‚’ã„ãŸã ã‘ã‚Œã°ã€ãŠç­”ãˆã§ãã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚"

        try:
            # æ¤œç´¢çµæœã‹ã‚‰é–¢é€£æƒ…å ±ã‚’æŠ½å‡º
            context_info = self._prepare_context(search_results)

            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
            prompt = self._build_prompt(query, context_info, context)

            # OpenAI APIã‚’å‘¼ã³å‡ºã—
            response = self._call_llm(prompt)

            return response

        except Exception as e:
            logger.error(f"LLM error: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self._generate_simple_answer(query, search_results)

    def _prepare_context(self, search_results: List[Dict[str, Any]]) -> str:
        """
        æ¤œç´¢çµæœã‹ã‚‰æ–‡è„ˆæƒ…å ±ã‚’æº–å‚™

        Args:
            search_results: æ¤œç´¢çµæœ

        Returns:
            æ–‡è„ˆæƒ…å ±ã®æ–‡å­—åˆ—
        """
        context_parts = []

        for i, result in enumerate(search_results[:3], 1):  # ä¸Šä½3ä»¶ã‚’ä½¿ç”¨
            # SearchHitã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å±æ€§ã‚’å–å¾—
            text = result.text if hasattr(result, 'text') else result.get('text', '')
            file_name = result.file_name if hasattr(result, 'file_name') else result.get('file_name', '')
            page_no = result.page_no if hasattr(result, 'page_no') else result.get('page_no', 0)
            section = result.section if hasattr(result, 'section') else result.get('section', '')

            # æ–‡è„ˆã‚’æ§‹ç¯‰
            context_parts.append(f"ã€å‚ç…§{i}ã€‘")
            context_parts.append(f"æ–‡æ›¸: {file_name} - ãƒšãƒ¼ã‚¸ {page_no}")
            if section:
                context_parts.append(f"ã‚»ã‚¯ã‚·ãƒ§ãƒ³: {section}")
            context_parts.append(f"å†…å®¹: {text[:500]}...")  # æœ€åˆã®500æ–‡å­—
            context_parts.append("")

        return "\n".join(context_parts)

    def _build_prompt(
        self,
        query: str,
        context_info: str,
        previous_context: Optional[str] = None
    ) -> str:
        """
        LLMç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰

        Args:
            query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¯ã‚¨ãƒª
            context_info: æ¤œç´¢çµæœã®æ–‡è„ˆæƒ…å ±
            previous_context: å‰ã®ä¼šè©±ã®æ–‡è„ˆ

        Returns:
            ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ–‡å­—åˆ—
        """
        system_prompt = """ã‚ãªãŸã¯ç¤¾å†…è¦ç¨‹ã«è©³ã—ã„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦å›ç­”ã—ã¦ãã ã•ã„ï¼š

1. æä¾›ã•ã‚ŒãŸå‚ç…§æƒ…å ±ã«åŸºã¥ã„ã¦æ­£ç¢ºã«å›ç­”ã™ã‚‹
2. è¦ç¨‹ã®å†…å®¹ã‚’æ­£ç¢ºã«ä¼ãˆã€è§£é‡ˆã‚„æ¨æ¸¬ã¯é¿ã‘ã‚‹
3. è©²å½“ã™ã‚‹æ¡ä»¶ã‚„ä¾‹å¤–äº‹é …ãŒã‚ã‚Œã°æ˜ç¢ºã«ç¤ºã™
4. å›ç­”ã¯ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ãã€ç®‡æ¡æ›¸ãã‚’æ´»ç”¨ã™ã‚‹
5. å‚ç…§ã—ãŸè¦ç¨‹åã¨ãƒšãƒ¼ã‚¸ç•ªå·ã‚’æ˜è¨˜ã™ã‚‹
6. ä¸æ˜ãªç‚¹ã¯ã€Œè¦ç¨‹ã«è¨˜è¼‰ãŒãªã„ã€ã¨æ­£ç›´ã«ä¼ãˆã‚‹"""

        user_prompt = f"""ã€è³ªå•ã€‘
{query}

ã€å‚ç…§æƒ…å ±ã€‘
{context_info}

ã€å›ç­”è¦ä»¶ã€‘
- å‚ç…§æƒ…å ±ã«åŸºã¥ã„ã¦æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„
- è©²å½“ã™ã‚‹æ¡ä»¶ã‚„åˆ¶é™äº‹é …ãŒã‚ã‚Œã°æ˜è¨˜ã—ã¦ãã ã•ã„
- å‚ç…§å…ƒï¼ˆæ–‡æ›¸åãƒ»ãƒšãƒ¼ã‚¸ï¼‰ã‚’å«ã‚ã¦ãã ã•ã„"""

        if previous_context:
            user_prompt = f"ã€å‰ã®æ–‡è„ˆã€‘\n{previous_context}\n\n" + user_prompt

        return system_prompt, user_prompt

    def _call_llm(self, prompt: tuple) -> str:
        """
        LLM APIã‚’å‘¼ã³å‡ºã—ã¦å›ç­”ã‚’ç”Ÿæˆ

        Args:
            prompt: (system_prompt, user_prompt)ã®ã‚¿ãƒ—ãƒ«

        Returns:
            ç”Ÿæˆã•ã‚ŒãŸå›ç­”
        """
        system_prompt, user_prompt = prompt

        try:
            response = self.client.chat.completions.create(
                model=self.config.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens
            )

            return response.choices[0].message.content

        except Exception as e:
            logger.error(f"OpenAI API error: {e}")
            raise

    def _generate_simple_answer(
        self,
        query: str,
        search_results: List[Dict[str, Any]]
    ) -> str:
        """
        LLMã‚’ä½¿ã‚ãªã„ç°¡æ˜“çš„ãªå›ç­”ã‚’ç”Ÿæˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰

        Args:
            query: ã‚¯ã‚¨ãƒª
            search_results: æ¤œç´¢çµæœ

        Returns:
            ç°¡æ˜“çš„ãªå›ç­”
        """
        if not search_results:
            return "è©²å½“ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"

        # æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„çµæœã‚’å–å¾—
        best_result = search_results[0]

        text = best_result.text if hasattr(best_result, 'text') else best_result.get('text', '')
        file_name = best_result.file_name if hasattr(best_result, 'file_name') else best_result.get('file_name', '')
        page_no = best_result.page_no if hasattr(best_result, 'page_no') else best_result.get('page_no', 0)

        # ãƒ†ã‚­ã‚¹ãƒˆã®æœ€åˆã®200æ–‡å­—ã‚’æŠ½å‡º
        summary = text[:200] + "..." if len(text) > 200 else text

        answer = f"ğŸ“„ **é–¢é€£æƒ…å ±**\n\n{summary}\n\n"
        answer += f"ğŸ“š å‡ºå…¸: {file_name} - ãƒšãƒ¼ã‚¸ {page_no}"

        return answer


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼‰
_llm_generator = None


def get_llm_generator() -> Optional[LLMAnswerGenerator]:
    """LLMå›ç­”ç”Ÿæˆå™¨ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _llm_generator
    if _llm_generator is None:
        try:
            _llm_generator = LLMAnswerGenerator()
        except Exception as e:
            logger.warning(f"Failed to initialize LLM generator: {e}")
            return None
    return _llm_generator


def generate_llm_answer(
    query: str,
    search_results: List[Any],
    context: Optional[str] = None,
    use_llm: bool = True
) -> str:
    """
    LLMã‚’ä½¿ç”¨ã—ã¦å›ç­”ã‚’ç”Ÿæˆï¼ˆã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆï¼‰

    Args:
        query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¯ã‚¨ãƒª
        search_results: æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
        context: å‰ã®ä¼šè©±ã®æ–‡è„ˆ
        use_llm: LLMã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹

    Returns:
        ç”Ÿæˆã•ã‚ŒãŸå›ç­”
    """
    if not use_llm:
        # LLMã‚’ä½¿ã‚ãªã„å ´åˆã¯æ—¢å­˜ã®å®Ÿè£…ã‚’ä½¿ç”¨
        from pdf.intelligent_answer import generate_intelligent_answer
        return generate_intelligent_answer(query, search_results, context)

    # LLMç”Ÿæˆå™¨ã‚’å–å¾—
    generator = get_llm_generator()

    if generator is None or not generator.enabled:
        # LLMãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯æ—¢å­˜ã®å®Ÿè£…ã‚’ä½¿ç”¨
        from pdf.intelligent_answer import generate_intelligent_answer
        return generate_intelligent_answer(query, search_results, context)

    # LLMã§å›ç­”ã‚’ç”Ÿæˆ
    return generator.generate_answer(query, search_results, context)