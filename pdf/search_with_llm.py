"""
LLMçµ±åˆæ¤œç´¢ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
æ¤œç´¢çµæœã‚’åŸºã«LLMã§è‡ªç„¶ãªå›ç­”ã‚’ç”Ÿæˆã™ã‚‹çµ±åˆæ©Ÿèƒ½
"""
from typing import List, Optional, Dict, Any, Tuple
from pathlib import Path
from dataclasses import dataclass
from loguru import logger

from .search import search, SearchHit
from .llm_answer import generate_llm_answer
from .intelligent_answer import generate_intelligent_answer
from .snippet import make_snippet
from core.config import AppConfig


@dataclass
class SearchWithLLMResult:
    """
    LLMçµ±åˆæ¤œç´¢ã®çµæœ
    """
    query: str
    answer: str
    search_hits: List[SearchHit]
    snippet: str
    confidence: str
    sources: List[Dict[str, Any]]
    llm_used: bool


def search_with_llm(
    query: str,
    top_k: int = 5,
    index_path: Path | str = "./data/index.sqlite",
    min_score: float = 30.0,
    context: Optional[str] = None,
    use_llm: Optional[bool] = None
) -> SearchWithLLMResult:
    """
    æ¤œç´¢ã¨LLMå›ç­”ç”Ÿæˆã‚’çµ±åˆã—ãŸé–¢æ•°

    Args:
        query: æ¤œç´¢ã‚¯ã‚¨ãƒª
        top_k: è¿”ã™çµæœã®æœ€å¤§æ•°
        index_path: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ‘ã‚¹
        min_score: æœ€å°ã‚¹ã‚³ã‚¢é–¾å€¤
        context: å‰ã®ä¼šè©±ã®æ–‡è„ˆ
        use_llm: LLMã‚’ä½¿ç”¨ã™ã‚‹ã‹ï¼ˆNoneã®å ´åˆã¯è¨­å®šã‹ã‚‰åˆ¤æ–­ï¼‰

    Returns:
        SearchWithLLMResult: çµ±åˆã•ã‚ŒãŸæ¤œç´¢çµæœ
    """
    logger.info(f"Searching with LLM for: {query[:50]}...")

    # æ¤œç´¢ã‚’å®Ÿè¡Œ
    search_hits = search(
        query=query,
        top_k=top_k,
        index_path=index_path,
        min_score=min_score,
        context=context
    )

    if not search_hits:
        logger.warning("No search results found")
        return SearchWithLLMResult(
            query=query,
            answer="ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚è©²å½“ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚åˆ¥ã®è¡¨ç¾ã§ãŠå°‹ã­ãã ã•ã„ã€‚",
            search_hits=[],
            snippet="",
            confidence="low",
            sources=[],
            llm_used=False
        )

    # LLMä½¿ç”¨ã®åˆ¤å®š
    if use_llm is None:
        # è¨­å®šã‹ã‚‰åˆ¤æ–­
        try:
            config = AppConfig.load()
            use_llm = config.llm_provider != "none"
        except Exception as e:
            logger.warning(f"Failed to load config: {e}")
            use_llm = False

    # å›ç­”ã‚’ç”Ÿæˆ
    llm_used = False
    if use_llm:
        try:
            # LLMã‚’ä½¿ç”¨ã—ã¦å›ç­”ã‚’ç”Ÿæˆ
            answer = generate_llm_answer(
                query=query,
                search_results=search_hits,
                context=context,
                use_llm=True
            )
            llm_used = True
            logger.info("Answer generated using LLM")
        except Exception as e:
            logger.error(f"Failed to generate LLM answer: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®å›ç­”ç”Ÿæˆ
            answer = generate_intelligent_answer(query, search_hits, context)
            logger.info("Fallback to rule-based answer generation")
    else:
        # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®å›ç­”ç”Ÿæˆ
        answer = generate_intelligent_answer(query, search_hits, context)
        logger.info("Answer generated using rule-based system")

    # ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’ç”Ÿæˆ
    best_hit = search_hits[0]
    snippet = make_snippet(best_hit.text, query, max_length=200)

    # ä¿¡é ¼åº¦ã‚’åˆ¤å®š
    confidence = _determine_confidence(search_hits)

    # ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’æ•´ç†
    sources = _prepare_sources(search_hits[:3])

    return SearchWithLLMResult(
        query=query,
        answer=answer,
        search_hits=search_hits,
        snippet=snippet,
        confidence=confidence,
        sources=sources,
        llm_used=llm_used
    )


def _determine_confidence(hits: List[SearchHit]) -> str:
    """
    æ¤œç´¢çµæœã‹ã‚‰ä¿¡é ¼åº¦ã‚’åˆ¤å®š

    Args:
        hits: æ¤œç´¢çµæœãƒªã‚¹ãƒˆ

    Returns:
        ä¿¡é ¼åº¦ï¼ˆhigh/medium/lowï¼‰
    """
    if not hits:
        return "low"

    top_score = hits[0].score

    if top_score >= 100:
        return "high"
    elif top_score >= 60:
        return "medium"
    else:
        return "low"


def _prepare_sources(hits: List[SearchHit]) -> List[Dict[str, Any]]:
    """
    ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’æ•´ç†

    Args:
        hits: æ¤œç´¢çµæœï¼ˆä¸Šä½æ•°ä»¶ï¼‰

    Returns:
        ã‚½ãƒ¼ã‚¹æƒ…å ±ã®ãƒªã‚¹ãƒˆ
    """
    sources = []

    for hit in hits:
        source = {
            "file_name": hit.file_name,
            "page_no": hit.page_no,
            "score": hit.score,
            "section": hit.section,
            "preview": make_snippet(hit.text, "", max_length=100)
        }
        sources.append(source)

    return sources


def format_search_result(result: SearchWithLLMResult) -> str:
    """
    æ¤œç´¢çµæœã‚’æ•´å½¢ã—ã¦è¡¨ç¤ºç”¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ

    Args:
        result: æ¤œç´¢çµæœ

    Returns:
        æ•´å½¢ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    """
    formatted = []

    # å›ç­”
    formatted.append(f"**å›ç­”:**\n{result.answer}\n")

    # ã‚¹ãƒ‹ãƒšãƒƒãƒˆï¼ˆç°¡æ½”ãªå¼•ç”¨ï¼‰
    if result.snippet:
        formatted.append(f"ğŸ“Œ **é–¢é€£ç®‡æ‰€:**\n{result.snippet}\n")

    # ä¿¡é ¼åº¦
    confidence_emoji = {
        "high": "ğŸŸ¢",
        "medium": "ğŸŸ¡",
        "low": "ğŸ”´"
    }
    emoji = confidence_emoji.get(result.confidence, "âš«")
    formatted.append(f"{emoji} **ä¿¡é ¼åº¦:** {result.confidence}")

    # LLMä½¿ç”¨çŠ¶æ³
    if result.llm_used:
        formatted.append("ğŸ¤– *AIï¼ˆLLMï¼‰ã«ã‚ˆã‚‹å›ç­”*")
    else:
        formatted.append("ğŸ“‹ *ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã«ã‚ˆã‚‹å›ç­”*")

    # å‡ºå…¸
    if result.sources:
        formatted.append("\nğŸ“š **å‡ºå…¸:**")
        for i, source in enumerate(result.sources, 1):
            formatted.append(
                f"  {i}. {source['file_name']} - ãƒšãƒ¼ã‚¸ {source['page_no']}"
                + (f" ({source['section']})" if source['section'] else "")
            )

    return "\n".join(formatted)


def search_and_answer(
    query: str,
    context: Optional[str] = None,
    **kwargs
) -> Tuple[str, List[SearchHit]]:
    """
    æ¤œç´¢ã—ã¦å›ç­”ã‚’ç”Ÿæˆã™ã‚‹ç°¡æ˜“ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹

    Args:
        query: æ¤œç´¢ã‚¯ã‚¨ãƒª
        context: å‰ã®ä¼šè©±ã®æ–‡è„ˆ
        **kwargs: ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

    Returns:
        (å›ç­”ãƒ†ã‚­ã‚¹ãƒˆ, æ¤œç´¢çµæœ)ã®ã‚¿ãƒ—ãƒ«
    """
    result = search_with_llm(query, context=context, **kwargs)
    formatted_answer = format_search_result(result)
    return formatted_answer, result.search_hits