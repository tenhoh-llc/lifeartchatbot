"""
ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆå›ç­”ç”Ÿæˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
å®Ÿéš›ã®PDFå†…å®¹ã‚’è§£æã—ã¦é©åˆ‡ãªå›ç­”ã‚’ç”Ÿæˆ
"""
from typing import List, Optional, Dict, Any
import re
from loguru import logger


class IntelligentAnswerGenerator:
    """
    PDFå†…å®¹ã‹ã‚‰çŸ¥çš„ãªå›ç­”ã‚’ç”Ÿæˆã™ã‚‹ã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self):
        self.patterns = self._create_extraction_patterns()
    
    def _create_extraction_patterns(self) -> Dict[str, Dict[str, str]]:
        """
        æƒ…å ±æŠ½å‡ºç”¨ã®æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å®šç¾©
        """
        return {
            "è‚²å…ä¼‘æ¥­": {
                "æœŸé–“": r"([\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+æ­³(?:[\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã‹æœˆ)?(?:ã«æº€ãŸãªã„|ã¾ã§|ã«é”ã™ã‚‹))",
                "å¯¾è±¡è€…": r"è‚²å…ä¼‘æ¥­(?:ã®å¯¾è±¡è€…|ãŒã§ãã‚‹).{0,50}(ç¤¾å“¡|å¾“æ¥­å“¡|å¥‘ç´„ç¤¾å“¡)",
                "æ¡ä»¶": r"(å…¥ç¤¾[\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+å¹´ä»¥ä¸Š|é›‡ç”¨é–¢ä¿‚ãŒç¶™ç¶š|æ‰€å®šåŠ´åƒæ—¥æ•°)",
                "é™¤å¤–å¯¾è±¡": r"é™¤å¤–ã•ã‚ŒãŸ.{0,50}(ç¤¾å“¡|å…¥ç¤¾[\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+å¹´æœªæº€|é€±é–“ã®æ‰€å®šåŠ´åƒæ—¥æ•°ãŒ[\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+æ—¥ä»¥ä¸‹)",
                "ç”³è«‹": r"(ç”³å‡º|å±Šå‡º).{0,30}([\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã‹æœˆå‰|[\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+æ—¥å‰)",
            },
            "æœ‰çµ¦ä¼‘æš‡": {
                "ä»˜ä¸æ—¥æ•°": r"(æœ‰çµ¦|å¹´æ¬¡|å¹´ä¼‘).{0,30}([\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+æ—¥)",
                "ç¹°è¶Š": r"(ç¹°[è¶Šç¹°]|æŒ[è¶Šã¡]).{0,30}(å¯èƒ½|ã§ãã‚‹|ç¿Œå¹´)",
                "æœ€å¤§ä¿æœ‰": r"(æœ€å¤§|ä¸Šé™).{0,30}([\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+æ—¥)",
                "å–å¾—æ¡ä»¶": r"(ä»˜ä¸|å–å¾—).{0,20}(æ¡ä»¶|è¦ä»¶)",
            },
            "åŠ´åƒæ™‚é–“": {
                "æ‰€å®š": r"(æ‰€å®š|é€šå¸¸|æ¨™æº–).{0,20}(åŠ´åƒæ™‚é–“|å‹¤å‹™æ™‚é–“).{0,30}([\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+æ™‚é–“)",
                "æ™‚é–“å¤–ä¸Šé™": r"(æ™‚é–“å¤–|æ®‹æ¥­).{0,20}(ä¸Šé™|é™åº¦|ã¾ã§).{0,30}([\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+æ™‚é–“)",
                "ä¼‘æ†©": r"(ä¼‘æ†©).{0,30}([\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+åˆ†|[\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+æ™‚é–“)",
                "å‹¤å‹™æ™‚é–“": r"(å§‹æ¥­|çµ‚æ¥­|å‹¤å‹™).{0,20}([\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+æ™‚)",
            },
            "çµ¦ä¸": {
                "ç· æ—¥": r"(ç· [æ—¥ã‚]|ã€†).{0,30}([\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+æ—¥)",
                "æ”¯æ‰•æ—¥": r"(æ”¯[æ‰•çµ¦]|æŒ¯è¾¼).{0,30}([\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+æ—¥)",
                "è³ä¸": r"(è³ä¸|ãƒœãƒ¼ãƒŠã‚¹).{0,50}([\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+æœˆ|å¹´[\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+å›)",
            },
            "é€€è·": {
                "ç”³å‡ºæœŸé–“": r"(é€€è·).{0,30}([\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[æ—¥æœˆãƒ¶]å‰)",
                "æ‰‹ç¶šã": r"(é€€è·).{0,20}(æ‰‹ç¶š|å±Š|é¡˜)",
                "å¼•ç¶™ã": r"(å¼•[ç¶™ã]|å¼•æ¸¡).{0,50}",
            }
        }
    
    def extract_information(self, text: str, topic: str) -> Dict[str, str]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç‰¹å®šãƒˆãƒ”ãƒƒã‚¯ã®æƒ…å ±ã‚’æŠ½å‡º
        
        Args:
            text: æ¤œç´¢å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            topic: ãƒˆãƒ”ãƒƒã‚¯ï¼ˆè‚²å…ä¼‘æ¥­ã€æœ‰çµ¦ä¼‘æš‡ãªã©ï¼‰
            
        Returns:
            æŠ½å‡ºã•ã‚ŒãŸæƒ…å ±ã®è¾æ›¸
        """
        extracted = {}
        
        if topic not in self.patterns:
            return extracted
        
        patterns = self.patterns[topic]
        
        for key, pattern in patterns.items():
            matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)
            if matches:
                # æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„ãƒãƒƒãƒã‚’é¸æŠ
                if isinstance(matches[0], tuple):
                    extracted[key] = " ".join(str(m) for m in matches[0] if m)
                else:
                    extracted[key] = matches[0]
        
        return extracted
    
    def identify_topic(self, query: str) -> Optional[str]:
        """
        ã‚¯ã‚¨ãƒªã‹ã‚‰ãƒˆãƒ”ãƒƒã‚¯ã‚’ç‰¹å®š
        
        Args:
            query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¯ã‚¨ãƒª
            
        Returns:
            ç‰¹å®šã•ã‚ŒãŸãƒˆãƒ”ãƒƒã‚¯
        """
        query_lower = query.lower()
        
        topic_keywords = {
            "è‚²å…ä¼‘æ¥­": ["è‚²å…", "è‚²ä¼‘", "ç”£ä¼‘", "å­è‚²ã¦", "å‡ºç”£"],
            "æœ‰çµ¦ä¼‘æš‡": ["æœ‰çµ¦", "æœ‰ä¼‘", "å¹´ä¼‘", "å¹´æ¬¡ä¼‘æš‡"],
            "åŠ´åƒæ™‚é–“": ["åŠ´åƒæ™‚é–“", "å‹¤å‹™æ™‚é–“", "æ®‹æ¥­", "æ™‚é–“å¤–", "æ‰€å®š"],
            "çµ¦ä¸": ["çµ¦ä¸", "çµ¦æ–™", "è³ƒé‡‘", "ç· æ—¥", "æ”¯æ‰•æ—¥", "è³ä¸"],
            "é€€è·": ["é€€è·", "è¾è·", "é›¢è·", "é€€ç¤¾"],
        }
        
        for topic, keywords in topic_keywords.items():
            if any(kw in query_lower for kw in keywords):
                return topic
        
        return None
    
    def analyze_negative_query(self, query: str, text: str) -> Dict[str, List[str]]:
        """
        å¦å®šçš„ãªè³ªå•ï¼ˆã€œã§ããªã„ã€å¯¾è±¡å¤–ãªã©ï¼‰ã‚’åˆ†æ
        
        Args:
            query: ã‚¯ã‚¨ãƒª
            text: ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            é™¤å¤–æ¡ä»¶ãªã©ã®æƒ…å ±
        """
        result = {"exclusions": [], "conditions": []}
        
        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™
        exclusion_patterns = [
            r"æ¬¡[ã®ã«æ²ã’ã‚‹].{0,50}(è€…|å ´åˆ|ã¨ã)ã¯.{0,20}(é™¤ã|é™¤å¤–|å¯¾è±¡å¤–|é©ç”¨ã—ãªã„)",
            r"(ãŸã ã—|ä½†ã—).{0,100}(é™¤ã|é™¤å¤–|ã§ããªã„)",
            r"ä»¥ä¸‹[ã®ã«è©²å½“].{0,50}å ´åˆ.{0,20}(é™¤ã|é™¤å¤–|ä¸å¯)",
            r"([\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+).{0,10}(é™¤å¤–|å¯¾è±¡å¤–|é©ç”¨ã—ãªã„)",
        ]
        
        for pattern in exclusion_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)
            for match in matches:
                if isinstance(match, tuple):
                    result["exclusions"].append(" ".join(str(m) for m in match if m))
                else:
                    result["exclusions"].append(match)
        
        # ä¸€èˆ¬çš„ãªé™¤å¤–æ¡ä»¶ã‚’æ¢ã™
        common_exclusions = [
            ("é›‡ç”¨æœŸé–“", r"é›‡ç”¨.{0,10}æœŸé–“.{0,30}([\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[å¹´æœˆæ—¥]|æœªæº€|ä»¥[ä¸Šä¸‹å†…])"),
            ("åŠ´åƒæ—¥æ•°", r"(é€±|æœˆ).{0,10}(æ‰€å®š)?åŠ´åƒæ—¥.{0,20}([\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+æ—¥)"),
            ("é›‡ç”¨å½¢æ…‹", r"(æ—¥ã€…é›‡ç”¨|æ—¥é›‡|è‡¨æ™‚|è©¦ç”¨æœŸé–“)"),
        ]
        
        for label, pattern in common_exclusions:
            if re.search(pattern, text, re.IGNORECASE):
                result["conditions"].append(label)
        
        return result
    
    def generate_answer(
        self,
        query: str,
        hits: List[Any],  # SearchHitã¾ãŸã¯äº’æ›ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        context: Optional[str] = None
    ) -> str:
        """
        ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãªå›ç­”ã‚’ç”Ÿæˆ
        
        Args:
            query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¯ã‚¨ãƒª
            hits: æ¤œç´¢çµæœ
            context: å‰ã®ä¼šè©±ã®æ–‡è„ˆ
            
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸå›ç­”
        """
        if not hits:
            return "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚è©²å½“ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        best_hit = hits[0]
        text = best_hit.text
        
        # ãƒˆãƒ”ãƒƒã‚¯ã‚’ç‰¹å®š
        topic = self.identify_topic(query)
        if context and not topic:
            topic = self.identify_topic(context)
        
        # å¦å®šçš„ãªè³ªå•ã‹ãƒã‚§ãƒƒã‚¯
        is_negative = any(word in query for word in ["å–ã‚Œãªã„", "ã§ããªã„", "å¯¾è±¡å¤–", "é™¤å¤–", "ä¾‹å¤–"])
        
        answer_parts = []
        
        if is_negative:
            # é™¤å¤–æ¡ä»¶ã‚’æ¢ã™
            negative_info = self.analyze_negative_query(query, text)
            
            if negative_info["exclusions"] or negative_info["conditions"]:
                answer_parts.append(f"ğŸš« **{topic or 'è©²å½“'}ã®é™¤å¤–æ¡ä»¶ãƒ»åˆ¶é™**\n")
                
                # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å…·ä½“çš„ãªé™¤å¤–æ¡ä»¶ã‚’æŠ½å‡º
                for exclusion in negative_info["exclusions"]:
                    answer_parts.append(f"â€¢ {exclusion}")
                
                # ä¸€èˆ¬çš„ãªæ¡ä»¶
                if negative_info["conditions"]:
                    answer_parts.append("\n**ä¸€èˆ¬çš„ãªåˆ¶é™äº‹é …:**")
                    for condition in negative_info["conditions"]:
                        answer_parts.append(f"â€¢ {condition}ã«é–¢ã™ã‚‹åˆ¶é™ã‚ã‚Š")
            else:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®é™¤å¤–æ¡ä»¶
                answer_parts.append(f"ğŸš« **ä¸€èˆ¬çš„ãªé™¤å¤–æ¡ä»¶**\n")
                answer_parts.append("â€» è©³ç´°ã¯è©²å½“è¦ç¨‹ã‚’ã”ç¢ºèªãã ã•ã„")
        
        else:
            # é€šå¸¸ã®è³ªå•
            if topic:
                # ãƒˆãƒ”ãƒƒã‚¯ã«å¿œã˜ãŸæƒ…å ±æŠ½å‡º
                extracted = self.extract_information(text, topic)
                
                # ã‚¢ã‚¤ã‚³ãƒ³ã‚’é¸æŠ
                icons = {
                    "è‚²å…ä¼‘æ¥­": "ğŸ‘¶",
                    "æœ‰çµ¦ä¼‘æš‡": "ğŸ–ï¸",
                    "åŠ´åƒæ™‚é–“": "â°",
                    "çµ¦ä¸": "ğŸ’°",
                    "é€€è·": "ğŸ“"
                }
                icon = icons.get(topic, "ğŸ“„")
                
                answer_parts.append(f"{icon} **{topic}ã«ã¤ã„ã¦**\n")
                
                # æƒ…å ±æŠ½å‡ºã‚’è©¦ã¿ã‚‹
                extracted = self.extract_information(text, topic)
                
                # æŠ½å‡ºã•ã‚ŒãŸæƒ…å ±ãŒã‚ã‚Œã°è¡¨ç¤ºã€ãªã‘ã‚Œã°è¦ç´„ã‚’ä½œæˆ
                if extracted:
                    # æŠ½å‡ºã•ã‚ŒãŸæƒ…å ±ã‚’æ•´å½¢
                    for key, value in extracted.items():
                        if value and len(value) > 5:  # çŸ­ã™ãã‚‹å€¤ã¯é™¤å¤–
                            formatted_key = key.replace("_", " ").title()
                            answer_parts.append(f"â€¢ {formatted_key}: {value}")
                
                # è¦ç´„ã‚‚è¿½åŠ ï¼ˆæŠ½å‡ºæƒ…å ±ã®è£œå®Œã¨ã—ã¦ï¼‰
                summary = self._create_summary(text, query)
                if summary:
                    answer_parts.append(f"\n**è©³ç´°æƒ…å ±:**\n{summary}")
            else:
                # ãƒˆãƒ”ãƒƒã‚¯ä¸æ˜ã®å ´åˆã¯è¦ç´„
                summary = self._create_summary(text, query)
                answer_parts.append(f"ğŸ“„ **é–¢é€£æƒ…å ±**\n{summary}")
        
        # å›ç­”ã‚’çµåˆ
        answer = "\n".join(answer_parts)
        
        # æ³¨æ„æ›¸ãã‚’è¿½åŠ 
        if not answer_parts or "è©³ç´°ã¯" not in answer:
            answer += "\n\nâ€» è©³ç´°ã¯è©²å½“è¦ç¨‹ã‚’ã”ç¢ºèªãã ã•ã„"
        
        return answer
    
    def _extract_additional_info(self, text: str, topic: str) -> str:
        """
        è¿½åŠ æƒ…å ±ã‚’æŠ½å‡º
        """
        # ãƒˆãƒ”ãƒƒã‚¯ã”ã¨ã®è¿½åŠ æƒ…å ±ãƒ‘ã‚¿ãƒ¼ãƒ³
        additional_patterns = {
            "è‚²å…ä¼‘æ¥­": [
                (r"ç”³[è«‹å‡º].{0,30}([\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[æ—¥æœˆãƒ¶]å‰)", "ç”³è«‹æœŸé™: {}"),
                (r"å¿…è¦æ›¸é¡.{0,50}", "{}"),
            ],
            "æœ‰çµ¦ä¼‘æš‡": [
                (r"å–å¾—å˜ä½.{0,30}(åŠæ—¥|æ™‚é–“|æ—¥)", "å–å¾—å˜ä½: {}"),
                (r"ç”³è«‹.{0,30}([\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+æ—¥å‰)", "ç”³è«‹: {}"),
            ],
        }
        
        if topic not in additional_patterns:
            return ""
        
        info_parts = []
        for pattern, format_str in additional_patterns[topic]:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                if match.groups():
                    info_parts.append(format_str.format(match.group(1)))
                else:
                    info_parts.append(format_str.format(match.group(0)))
        
        return "\n".join(info_parts)
    
    def _create_summary(self, text: str, query: str) -> str:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã®è¦ç´„ã‚’ä½œæˆï¼ˆæ”¹å–„ç‰ˆï¼‰
        """
        # ã‚ˆã‚Šé©åˆ‡ãªæ–‡ã®åˆ†å‰²
        import re
        sentences = re.split(r'[ã€‚\n]', text)
        sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 10]
        
        # ã‚¯ã‚¨ãƒªã‹ã‚‰é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        from pdf.search import extract_keywords
        keywords = extract_keywords(query)
        
        # é–¢é€£åº¦ã®é«˜ã„æ–‡ã‚’æŠ½å‡º
        scored_sentences = []
        for sentence in sentences[:20]:  # æœ€åˆã®20æ–‡ã‚’ãƒã‚§ãƒƒã‚¯
            score = 0
            for keyword in keywords:
                if keyword in sentence:
                    score += 1
            if score > 0:
                scored_sentences.append((score, sentence))
        
        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        scored_sentences.sort(key=lambda x: x[0], reverse=True)
        
        if scored_sentences:
            # æœ€ã‚‚é–¢é€£åº¦ã®é«˜ã„2-3æ–‡ã‚’é¸æŠ
            result_sentences = [s[1] for s in scored_sentences[:3]]
            # å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã§ã®å‡ºç¾é †ã«ä¸¦ã¹æ›¿ãˆ
            result_sentences.sort(key=lambda s: text.find(s))
            return "ã€‚".join(result_sentences) + "ã€‚"
        else:
            # é–¢é€£æ–‡ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€è¦ç¨‹ã®ä¸»è¦éƒ¨åˆ†ã‚’æŠ½å‡º
            important_patterns = [
                r'ç¬¬[\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+æ¡.*?ãŒã§ãã‚‹',
                r'å¸Œæœ›ã™ã‚‹.*?ã¨ã™ã‚‹',
                r'ç”³å‡º.*?ã§ãã‚‹',
                r'å¯¾è±¡.*?ã¨ã™ã‚‹'
            ]
            
            for pattern in important_patterns:
                match = re.search(pattern, text, re.DOTALL)
                if match:
                    return match.group(0)[:200]
            
            # ãã‚Œã§ã‚‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ€åˆã®æ–‡ã‚’è¿”ã™
            return sentences[0] if sentences else text[:200]


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_answer_generator = None


def get_answer_generator() -> IntelligentAnswerGenerator:
    """å›ç­”ç”Ÿæˆå™¨ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _answer_generator
    if _answer_generator is None:
        _answer_generator = IntelligentAnswerGenerator()
    return _answer_generator


def generate_intelligent_answer(
    query: str,
    hits: List[Any],  # SearchHitã¾ãŸã¯äº’æ›ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    context: Optional[str] = None
) -> str:
    """
    ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãªå›ç­”ã‚’ç”Ÿæˆï¼ˆã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆï¼‰
    """
    generator = get_answer_generator()
    return generator.generate_answer(query, hits, context)