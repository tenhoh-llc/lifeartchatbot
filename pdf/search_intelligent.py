"""
ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆæ¤œç´¢ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
æ–‡è„ˆç†è§£ã¨æ¡æ–‡å„ªå…ˆé †ä½ã‚’è€ƒæ…®ã—ãŸé«˜ç²¾åº¦æ¤œç´¢
"""
from dataclasses import dataclass
from typing import List, Optional, Set, Dict, Tuple
import sqlite3
from pathlib import Path
from rapidfuzz import fuzz
import re
from loguru import logger


@dataclass
class SearchResult:
    """æ¤œç´¢çµæœ"""
    file_name: str
    file_path: str
    page_no: int
    score: float
    text: str
    section: Optional[str]
    matched_terms: List[str]
    relevance_type: str  # é–¢é€£æ€§ã®ã‚¿ã‚¤ãƒ—ï¼ˆdefinition/condition/procedureç­‰ï¼‰


# åŒç¾©èªãƒ»ç•¥èªè¾æ›¸
SYNONYM_DICT = {
    "è‚²ä¼‘": ["è‚²å…ä¼‘æ¥­", "è‚²å…ä¼‘æš‡"],
    "ç”£ä¼‘": ["ç”£å‰ç”£å¾Œä¼‘æ¥­", "ç”£å‰ç”£å¾Œä¼‘æš‡", "å‡ºç”£ä¼‘æš‡"],
    "æœ‰ä¼‘": ["æœ‰çµ¦ä¼‘æš‡", "å¹´æ¬¡æœ‰çµ¦ä¼‘æš‡"],
    "æœ‰çµ¦": ["æœ‰çµ¦ä¼‘æš‡", "å¹´æ¬¡æœ‰çµ¦ä¼‘æš‡", "æœ‰ä¼‘"],  # æœ‰çµ¦ã‚‚è¿½åŠ 
    "ä»‹è­·ä¼‘": ["ä»‹è­·ä¼‘æ¥­", "ä»‹è­·ä¼‘æš‡"],
    "æ™‚çŸ­": ["çŸ­æ™‚é–“å‹¤å‹™", "æ™‚çŸ­å‹¤å‹™", "æ™‚é–“çŸ­ç¸®"],
    "æ®‹æ¥­": ["æ™‚é–“å¤–åŠ´åƒ", "æ™‚é–“å¤–å‹¤å‹™", "è¶…éå‹¤å‹™"],
    "ãƒ‘ãƒ¼ãƒˆ": ["ãƒ‘ãƒ¼ãƒˆã‚¿ã‚¤ãƒãƒ¼", "ãƒ‘ãƒ¼ãƒˆã‚¿ã‚¤ãƒ "],
}

# ã‚¯ã‚¨ãƒªæ„å›³ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
INTENT_PATTERNS = {
    "definition": ["ã¨ã¯", "ã«ã¤ã„ã¦", "æ•™ãˆã¦", "èª¬æ˜"],  # å®šç¾©ãƒ»æ¦‚è¦
    "condition": ["æ¡ä»¶", "è¦ä»¶", "å¯¾è±¡", "è³‡æ ¼", "ã§ãã‚‹"],  # æ¡ä»¶ãƒ»å¯¾è±¡è€…
    "procedure": ["æ‰‹ç¶šã", "ç”³è«‹", "æ–¹æ³•", "ã‚„ã‚Šæ–¹", "ã©ã†ã‚„ã£ã¦"],  # æ‰‹ç¶šã
    "period": ["æœŸé–“", "ã„ã¤ã¾ã§", "ä½•æ—¥", "ä½•ãƒ¶æœˆ", "ä½•å¹´"],  # æœŸé–“
    "benefit": ["çµ¦ä»˜", "æ‰‹å½“", "çµ¦ä¸", "ãŠé‡‘", "æ”¯çµ¦"],  # çµ¦ä»˜ãƒ»æ‰‹å½“
}

# æ¡æ–‡ã®é‡è¦åº¦ï¼ˆè‹¥ã„ç•ªå·ã»ã©åŸºæœ¬çš„ãªå†…å®¹ï¼‰
ARTICLE_IMPORTANCE = {
    "ç›®çš„": 100,  # ç¬¬1æ¡ï¼ˆç›®çš„ï¼‰
    "å®šç¾©": 95,   # å®šç¾©æ¡é …
    "å¯¾è±¡": 90,   # ç¬¬2æ¡ï¼ˆå¯¾è±¡è€…ï¼‰
    "æ¡ä»¶": 85,   # æ¡ä»¶ãƒ»è¦ä»¶
    "æœŸé–“": 70,   # æœŸé–“
    "æ‰‹ç¶š": 60,   # æ‰‹ç¶šã
    "ç”³è«‹": 60,   # ç”³è«‹
    "çµ¦ä»˜": 50,   # çµ¦ä»˜
}


def expand_query(query: str) -> Set[str]:
    """ã‚¯ã‚¨ãƒªã‚’æ‹¡å¼µï¼ˆç•¥èªâ†’æ­£å¼åç§°ï¼‰"""
    expanded = {query}
    query_lower = query.lower()
    
    # ã¾ãšå…ƒã®ã‚¯ã‚¨ãƒªã§æ‹¡å¼µ
    for short, longs in SYNONYM_DICT.items():
        if short in query_lower:
            for long_term in longs:
                expanded.add(query_lower.replace(short, long_term))
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ã§ã‚‚æ‹¡å¼µï¼ˆã€Œã«ã¤ã„ã¦æ•™ãˆã¦ã€ãªã©ã‚’é™¤å»ï¼‰
    # æ„å›³ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å»
    clean_query = query_lower
    for pattern_list in INTENT_PATTERNS.values():
        for pattern in pattern_list:
            clean_query = clean_query.replace(pattern, "").strip()
    clean_query = clean_query.replace("ï¼Ÿ", "").replace("?", "").strip()
    
    if clean_query != query_lower and clean_query:
        expanded.add(clean_query)
        # ã‚¯ãƒªãƒ¼ãƒ³ãªã‚¯ã‚¨ãƒªã§ã‚‚åŒç¾©èªå±•é–‹
        for short, longs in SYNONYM_DICT.items():
            if short in clean_query:
                for long_term in longs:
                    expanded.add(clean_query.replace(short, long_term))
    
    return expanded


def analyze_query_intent(query: str) -> str:
    """
    ã‚¯ã‚¨ãƒªã®æ„å›³ã‚’åˆ†æ
    
    Returns:
        æ„å›³ã®ã‚¿ã‚¤ãƒ—ï¼ˆdefinition/condition/procedure/period/benefit/generalï¼‰
    """
    query_lower = query.lower()
    
    for intent_type, keywords in INTENT_PATTERNS.items():
        if any(keyword in query_lower for keyword in keywords):
            return intent_type
    
    # ã€Œã«ã¤ã„ã¦ã€ã€Œæ•™ãˆã¦ã€ãŒã‚ˆãä½¿ã‚ã‚Œã‚‹ã®ã§ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å®šç¾©
    if "ï¼Ÿ" in query or "?" in query:
        return "definition"
    
    return "general"


def extract_article_info(text: str) -> Tuple[Optional[int], str]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ¡æ–‡ç•ªå·ã¨ç¨®é¡ã‚’æŠ½å‡º
    
    Returns:
        (æ¡æ–‡ç•ªå·, æ¡æ–‡ã‚¿ã‚¤ãƒ—)
    """
    # ã‚ˆã‚Šåºƒç¯„å›²ã§ç¬¬â—‹æ¡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™
    article_patterns = [
        r'ç¬¬([ï¼-ï¼™0-9]{1,3})æ¡',  # æ•°å­—ï¼ˆå…¨è§’ãƒ»åŠè§’ï¼‰
        r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]{1,3})æ¡',  # æ¼¢æ•°å­—
    ]
    
    article_num = None
    # ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‚’æ¢ç´¢ï¼ˆãŸã ã—æœ€åˆã®1000æ–‡å­—ã¾ã§ï¼‰
    search_text = text[:1000]
    
    for pattern in article_patterns:
        matches = re.findall(pattern, search_text)
        if matches:
            article_str = matches[0]
            
            # æ¡æ–‡ç•ªå·ã‚’æ•°å€¤ã«å¤‰æ›ï¼ˆæ‹¡å¼µç‰ˆï¼‰
            article_num_map = {
                'ï¼‘': 1, 'ï¼’': 2, 'ï¼“': 3, 'ï¼”': 4, 'ï¼•': 5,
                'ï¼–': 6, 'ï¼—': 7, 'ï¼˜': 8, 'ï¼™': 9, 'ï¼‘ï¼': 10,
                '1': 1, '2': 2, '3': 3, '4': 4, '5': 5,
                '6': 6, '7': 7, '8': 8, '9': 9, '10': 10,
                'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5,
                'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9, 'å': 10,
            }
            
            # 2æ¡ä»¥ä¸Šã®æ•°å­—ã®å‡¦ç†
            if article_str.isdigit():
                article_num = int(article_str)
            else:
                article_num = article_num_map.get(article_str, None)
                
                # 30ç•ªå°ã€40ç•ªå°ãªã©ã®å‡¦ç†
                if article_num is None and len(article_str) >= 2:
                    try:
                        article_num = int(article_str)
                    except:
                        article_num = 99  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            if article_num:
                break
    
    # æ¡æ–‡ã®ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
    text_preview = search_text.lower()
    article_type = "general"
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã§ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
    if 'ç›®çš„' in text_preview[:200]:
        article_type = "ç›®çš„"
    elif 'å¹´æ¬¡æœ‰çµ¦ä¼‘æš‡' in text_preview or 'æœ‰çµ¦ä¼‘æš‡' in text_preview:
        article_type = "ä¼‘æš‡"  # æœ‰çµ¦ä¼‘æš‡é–¢é€£
    elif 'è‚²å…ä¼‘æ¥­' in text_preview or 'ä»‹è­·ä¼‘æ¥­' in text_preview:
        article_type = "ä¼‘æ¥­"  # ä¼‘æ¥­é–¢é€£
    elif 'å¯¾è±¡' in text_preview or 'ã§ãã‚‹' in text_preview:
        article_type = "å¯¾è±¡"
    elif 'æ‰‹ç¶š' in text_preview or 'ç”³è«‹' in text_preview or 'ç”³å‡º' in text_preview:
        article_type = "æ‰‹ç¶š"
    elif 'æœŸé–“' in text_preview:
        article_type = "æœŸé–“"
    
    return (article_num if article_num else None, article_type)


def calculate_intelligent_score(
    query: str,
    text: str,
    file_name: str,
    section: Optional[str],
    page_no: int
) -> Tuple[float, List[str], str]:
    """
    ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆã‚¹ã‚³ã‚¢è¨ˆç®—
    
    Returns:
        (ã‚¹ã‚³ã‚¢, ãƒãƒƒãƒã—ãŸç”¨èªãƒªã‚¹ãƒˆ, é–¢é€£æ€§ã‚¿ã‚¤ãƒ—)
    """
    text_lower = text.lower()
    query_lower = query.lower()
    
    # 1. ã‚¯ã‚¨ãƒªæ‹¡å¼µã¨åŸºæœ¬ã‚¹ã‚³ã‚¢
    expanded_queries = expand_query(query)
    max_score = 0
    matched_terms = []
    
    for exp_query in expanded_queries:
        # éƒ¨åˆ†ä¸€è‡´ã‚¹ã‚³ã‚¢
        score = fuzz.partial_ratio(exp_query, text_lower)
        
        # å®Œå…¨ä¸€è‡´ãƒœãƒ¼ãƒŠã‚¹ï¼ˆå¤§å¹…ã«å¢—ã‚„ã™ï¼‰
        if exp_query in text_lower:
            score += 50  # 20â†’50ã«å¢—åŠ 
            matched_terms.append(exp_query)
            
            # å‡ºç¾å›æ•°ã«å¿œã˜ã¦è¿½åŠ ãƒœãƒ¼ãƒŠã‚¹ï¼ˆãŸã ã—ä¸Šé™ã‚ã‚Šï¼‰
            count = text_lower.count(exp_query)
            if count > 1:
                score += min(count * 5, 30)  # æœ€å¤§30ç‚¹ã¾ã§
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
        keywords = re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+', exp_query)
        for keyword in keywords:
            if len(keyword) >= 2 and keyword in text_lower:
                score += 10  # 5â†’10ã«å¢—åŠ 
                if keyword not in matched_terms:
                    matched_terms.append(keyword)
        
        max_score = max(max_score, score)
    
    # 2. ã‚¯ã‚¨ãƒªæ„å›³ã«åŸºã¥ããƒœãƒ¼ãƒŠã‚¹
    intent = analyze_query_intent(query)
    article_num, article_type = extract_article_info(text)
    
    # æ„å›³ã¨æ¡æ–‡ã‚¿ã‚¤ãƒ—ã®ãƒãƒƒãƒãƒ³ã‚°
    intent_bonus = 0
    relevance_type = "general"
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå®Ÿéš›ã«ãƒ†ã‚­ã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    has_relevant_content = False
    for term in matched_terms:
        if term in text_lower:
            has_relevant_content = True
            break
    
    if intent == "definition" or intent == "condition":
        # å®šç¾©ã‚„æ¡ä»¶ã‚’æ±‚ã‚ã¦ã„ã‚‹å ´åˆ
        
        # ä¼‘æš‡é–¢é€£ã®æ¡æ–‡ã¯ç‰¹åˆ¥æ‰±ã„
        if article_type == "ä¼‘æš‡":
            intent_bonus = 60  # ä¼‘æš‡æ¡æ–‡ã¯é«˜ãƒœãƒ¼ãƒŠã‚¹
            relevance_type = "definition"
        elif article_type == "ä¼‘æ¥­" and "ä¼‘" in query_lower:
            intent_bonus = 50
            relevance_type = "definition"
        # ç¬¬1ã€œ3æ¡ã®å‡¦ç†ï¼ˆãŸã ã—é–¢é€£å†…å®¹ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
        elif article_num and article_num <= 3:
            if has_relevant_content:
                intent_bonus = 40  # ä¸‹ã’ã‚‹ï¼ˆ50â†’40ï¼‰
                relevance_type = "definition"
            else:
                intent_bonus = 5  # å¤§å¹…ã«ä¸‹ã’ã‚‹ï¼ˆ20â†’5ï¼‰
        
        # ç›®çš„ãƒ»å¯¾è±¡ãƒ»å®šç¾©ã‚¿ã‚¤ãƒ—ã®å‡¦ç†
        if article_type in ["ç›®çš„", "å¯¾è±¡", "å®šç¾©"]:
            if has_relevant_content:
                intent_bonus += 20  # ä¸‹ã’ã‚‹ï¼ˆ30â†’20ï¼‰
            else:
                intent_bonus += 0  # é–¢é€£å†…å®¹ãŒãªã„ç›®çš„æ¡æ–‡ã¯ãƒœãƒ¼ãƒŠã‚¹ãªã—
            relevance_type = "definition"
        elif article_type == "æ‰‹ç¶š":
            intent_bonus -= 20  # æ‰‹ç¶šãã¯ä¸‹ã’ã‚‹
    
    elif intent == "procedure":
        # æ‰‹ç¶šãã‚’æ±‚ã‚ã¦ã„ã‚‹å ´åˆ
        if article_type == "æ‰‹ç¶š":
            intent_bonus = 40
            relevance_type = "procedure"
        elif article_type in ["ç›®çš„", "å¯¾è±¡"]:
            intent_bonus -= 10
    
    elif intent == "period":
        # æœŸé–“ã‚’æ±‚ã‚ã¦ã„ã‚‹å ´åˆ
        if "æœŸé–“" in text_lower or "æ—¥" in text_lower or "ãƒ¶æœˆ" in text_lower:
            intent_bonus = 30
            relevance_type = "period"
    
    # 3. æ¡æ–‡ç•ªå·ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘ï¼ˆå†…å®¹ã®é–¢é€£æ€§ã‚’é‡è¦–ï¼‰
    article_bonus = 0
    if article_num:
        # é–¢é€£å†…å®¹ãŒã‚ã‚‹å ´åˆã®ã¿ãƒœãƒ¼ãƒŠã‚¹
        if has_relevant_content:
            if article_num == 1 and article_type == "ç›®çš„":
                article_bonus = 10  # ç¬¬1æ¡ã§ã‚‚ç›®çš„ã®ã¿ãªã‚‰æ§ãˆã‚
            elif article_num == 2:
                article_bonus = 15  # ç¬¬2æ¡ï¼ˆå®šç¾©ãƒ»å¯¾è±¡ï¼‰
            elif article_num <= 5:
                article_bonus = 10
            elif article_num >= 30 and article_num <= 35:  # ä¼‘æš‡é–¢é€£ã®æ¡æ–‡ç•ªå·å¸¯
                if article_type == "ä¼‘æš‡":
                    article_bonus = 20  # ä¼‘æš‡ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æ¡æ–‡ã‚’å„ªé‡
            elif article_num > 50:
                article_bonus = -10  # å¾ŒåŠã®æ¡æ–‡ã¯å„ªå…ˆåº¦ä¸‹ã’ã‚‹
        else:
            # é–¢é€£å†…å®¹ãŒãªã„å ´åˆ
            if article_num == 1:
                article_bonus = -20  # ç¬¬1æ¡ã§ã‚‚é–¢é€£ãªã‘ã‚Œã°ãƒšãƒŠãƒ«ãƒ†ã‚£
    
    # 4. ãƒ•ã‚¡ã‚¤ãƒ«åã«ã‚ˆã‚‹èª¿æ•´
    file_bonus = 0
    if "è‚²" in query_lower or "è‚²ä¼‘" in query_lower:
        if "è‚²å…ä»‹è­·" in file_name:
            file_bonus = 30
        elif "ãƒ‘ãƒ¼ãƒˆ" in file_name:
            file_bonus = -30
    elif "ãƒ‘ãƒ¼ãƒˆ" in query_lower:
        if "ãƒ‘ãƒ¼ãƒˆ" in file_name:
            file_bonus = 30
        else:
            file_bonus = -20
    
    # 5. å˜èªå‡ºç¾é »åº¦ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆå¤šã™ãã‚‹å ´åˆã¯æ‰‹ç¶šãç³»ã®å¯èƒ½æ€§ï¼‰
    frequency_penalty = 0
    for term in matched_terms:
        count = text_lower.count(term.lower())
        if count > 15:  # 15å›ä»¥ä¸Šå‡ºç¾ã¯æ‰‹ç¶šãç³»ã®å¯èƒ½æ€§
            if intent != "procedure":  # æ‰‹ç¶šãã‚’æ±‚ã‚ã¦ã„ãªã„å ´åˆã¯ãƒšãƒŠãƒ«ãƒ†ã‚£
                frequency_penalty = -10
    
    # ç·åˆã‚¹ã‚³ã‚¢
    total_score = max_score + intent_bonus + article_bonus + file_bonus + frequency_penalty
    
    # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°
    logger.debug(f"""
    Page {page_no}: Score breakdown
    - Base: {max_score}
    - Intent bonus: {intent_bonus} (intent={intent}, article_type={article_type})
    - Article bonus: {article_bonus} (article_num={article_num})
    - File bonus: {file_bonus}
    - Frequency penalty: {frequency_penalty}
    - Total: {total_score}
    """)
    
    return max(0, total_score), matched_terms, relevance_type


def search_intelligent(
    query: str,
    index_path: Path = Path("./data/index.sqlite"),
    top_k: int = 5,
    min_score: int = 30
) -> List[SearchResult]:
    """
    ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆæ¤œç´¢
    
    Args:
        query: æ¤œç´¢ã‚¯ã‚¨ãƒª
        index_path: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹DBã®ãƒ‘ã‚¹
        top_k: è¿”ã™çµæœã®æœ€å¤§æ•°
        min_score: æœ€ä½ã‚¹ã‚³ã‚¢
        
    Returns:
        æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
    """
    if not query or not index_path.exists():
        return []
    
    results = []
    
    # ã‚¯ã‚¨ãƒªæ„å›³ã‚’åˆ†æ
    intent = analyze_query_intent(query)
    logger.info(f"Query intent: {intent} for query: {query}")
    
    with sqlite3.connect(index_path) as db:
        cursor = db.execute(
            "SELECT file_name, file_path, page_no, text, section FROM pages"
        )
        
        for file_name, file_path, page_no, text, section in cursor:
            score, matched_terms, relevance_type = calculate_intelligent_score(
                query, text, file_name, section, page_no
            )
            
            if score >= min_score:
                results.append(SearchResult(
                    file_name=file_name,
                    file_path=file_path,
                    page_no=page_no,
                    score=score,
                    text=text,
                    section=section,
                    matched_terms=matched_terms,
                    relevance_type=relevance_type
                ))
    
    # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
    results.sort(key=lambda x: x.score, reverse=True)
    
    # é–¢é€£æ€§ã‚¿ã‚¤ãƒ—ãŒåŒã˜ã‚‚ã®ã‚’å„ªå…ˆçš„ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    if results and intent in ["definition", "condition"]:
        # å®šç¾©ãƒ»æ¡ä»¶ã‚’æ±‚ã‚ã¦ã„ã‚‹å ´åˆã¯ã€ãã‚Œã‚‰ã‚’ä¸Šä½ã«
        definition_results = [r for r in results if r.relevance_type == "definition"]
        other_results = [r for r in results if r.relevance_type != "definition"]
        results = definition_results + other_results
    
    return results[:top_k]


def extract_intelligent_snippet(
    text: str,
    query: str,
    matched_terms: List[str],
    relevance_type: str,
    window: int = 200
) -> str:
    """
    ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆæŠœç²‹ç”Ÿæˆ
    é–¢é€£æ€§ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦æœ€é©ãªéƒ¨åˆ†ã‚’æŠ½å‡º
    """
    text_lower = text.lower()
    
    # æ¡æ–‡ã®é–‹å§‹ä½ç½®ã‚’æ¢ã™ï¼ˆå®šç¾©ãƒ»æ¡ä»¶ã®å ´åˆï¼‰
    if relevance_type == "definition":
        # ç¬¬â—‹æ¡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™
        article_pattern = re.search(r'ç¬¬[ï¼-ï¼™0-9ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+æ¡', text)
        if article_pattern:
            start_pos = article_pattern.start()
            # æ¡æ–‡ã®çµ‚ã‚ã‚Šã¾ã§ã‚’å«ã‚ã‚‹
            end_pos = min(len(text), start_pos + 400)
            excerpt = text[start_pos:end_pos]
            if end_pos < len(text):
                excerpt += "..."
            
            # ãƒãƒƒãƒç”¨èªã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ
            for term in matched_terms:
                pattern = re.compile(re.escape(term), re.IGNORECASE)
                excerpt = pattern.sub(f"**{term}**", excerpt)
            
            return excerpt
    
    # é€šå¸¸ã®æŠœç²‹ç”Ÿæˆ
    best_pos = -1
    best_term = query
    
    for term in matched_terms:
        pos = text_lower.find(term.lower())
        if pos != -1:
            best_pos = pos
            best_term = term
            break
    
    if best_pos == -1:
        best_pos = 0
    
    start = max(0, best_pos - window // 2)
    end = min(len(text), best_pos + len(best_term) + window // 2)
    
    excerpt = text[start:end]
    
    if start > 0:
        excerpt = "..." + excerpt
    if end < len(text):
        excerpt = excerpt + "..."
    
    # ãƒãƒƒãƒç”¨èªã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ
    for term in matched_terms:
        pattern = re.compile(re.escape(term), re.IGNORECASE)
        excerpt = pattern.sub(f"**{term}**", excerpt)
    
    return excerpt


def generate_intelligent_answer(query: str, results: List[SearchResult]) -> Dict:
    """
    ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆå›ç­”ç”Ÿæˆ
    """
    if not results:
        return {
            "found": False,
            "answer": "è©²å½“ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚",
            "suggestions": [
                "ã‚ˆã‚Šå…·ä½“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢ã—ã¦ãã ã•ã„",
                "è‚²å…ä¼‘æ¥­ã«é–¢ã—ã¦ã¯ã€Œè‚²ä¼‘ã€ã€Œè‚²å…ä¼‘æ¥­ã€ãªã©ã§æ¤œç´¢",
                "ãƒ‘ãƒ¼ãƒˆã‚¿ã‚¤ãƒãƒ¼ã«é–¢ã—ã¦ã¯ã€Œãƒ‘ãƒ¼ãƒˆã€ã€Œæ™‚çµ¦ã€ãªã©ã§æ¤œç´¢"
            ]
        }
    
    best_result = results[0]
    
    # ä¿¡é ¼åº¦ã®åˆ¤å®š
    confidence = "high" if best_result.score >= 100 else "medium" if best_result.score >= 70 else "low"
    
    # ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆæŠœç²‹
    excerpt = extract_intelligent_snippet(
        best_result.text,
        query,
        best_result.matched_terms,
        best_result.relevance_type,
        window=250
    )
    
    # ã‚¯ã‚¨ãƒªæ„å›³ã«å¿œã˜ãŸè¿½åŠ æƒ…å ±
    intent = analyze_query_intent(query)
    additional_info = ""
    
    if intent == "definition" and best_result.relevance_type == "definition":
        additional_info = "\n\nğŸ“Œ ã“ã®å†…å®¹ã¯è¦ç¨‹ã®åŸºæœ¬çš„ãªå®šç¾©ãƒ»æ¡ä»¶ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚"
    elif intent == "procedure" and best_result.relevance_type != "procedure":
        additional_info = "\n\nğŸ’¡ æ‰‹ç¶šãã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€ç”³è«‹ãƒ»æ‰‹ç¶šãã«é–¢ã™ã‚‹æ¡æ–‡ã‚‚ã”ç¢ºèªãã ã•ã„ã€‚"
    
    return {
        "found": True,
        "answer": excerpt + additional_info,
        "source": {
            "file": best_result.file_name,
            "page": best_result.page_no,
            "section": best_result.section,
            "score": best_result.score,
            "confidence": confidence,
            "relevance_type": best_result.relevance_type
        },
        "all_results": [
            {
                "file": result.file_name,
                "page": result.page_no,
                "score": result.score,
                "relevance_type": result.relevance_type
            }
            for result in results
        ]
    }


# ãƒ†ã‚¹ãƒˆ
if __name__ == "__main__":
    test_queries = [
        "è‚²ä¼‘ã«ã¤ã„ã¦æ•™ãˆã¦",
        "è‚²å…ä¼‘æ¥­ã®æ¡ä»¶ã¯ï¼Ÿ",
        "è‚²ä¼‘ã®æ‰‹ç¶šãæ–¹æ³•",
        "æœ‰çµ¦ä¼‘æš‡ã®ç¹°è¶Š",
        "ãƒ‘ãƒ¼ãƒˆã®å‹¤å‹™æ™‚é–“"
    ]
    
    for query in test_queries:
        print(f"\n{'='*50}")
        print(f"Query: {query}")
        print(f"Intent: {analyze_query_intent(query)}")
        
        results = search_intelligent(query, top_k=3)
        
        for i, result in enumerate(results[:2], 1):
            print(f"\n{i}. {result.file_name} - Page {result.page_no}")
            print(f"   Score: {result.score:.1f}")
            print(f"   Type: {result.relevance_type}")
            print(f"   Matched: {', '.join(result.matched_terms)}")