"""
LLMçµ±åˆæ¤œç´¢ã®ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰
"""
import os
import sys
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock

sys.path.insert(0, str(Path(__file__).parent))

from pdf.search import SearchHit
from pdf.llm_answer import LLMAnswerGenerator, LLMConfig, generate_llm_answer
from pdf.search_with_llm import search_with_llm, format_search_result, SearchWithLLMResult


def test_llm_answer_generator_without_api():
    """APIã‚­ãƒ¼ãªã—ã§LLMç”Ÿæˆå™¨ã‚’ãƒ†ã‚¹ãƒˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‹•ä½œï¼‰"""
    print("\n=== Test LLM Answer Generator Without API ===")

    # ãƒ¢ãƒƒã‚¯ã®æ¤œç´¢çµæœã‚’ä½œæˆ
    mock_hits = [
        SearchHit(
            file_name="å°±æ¥­è¦å‰‡.pdf",
            page_no=10,
            score=95.0,
            text="è‚²å…ä¼‘æ¥­ã¯ã€åŸå‰‡ã¨ã—ã¦å­ãŒ1æ­³ã«é”ã™ã‚‹ã¾ã§ã®é–“ã€å–å¾—ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ãŸã ã—ã€ä¿è‚²åœ’ã«å…¥ã‚Œãªã„ç­‰ã®äº‹æƒ…ãŒã‚ã‚‹å ´åˆã¯ã€æœ€é•·2æ­³ã¾ã§å»¶é•·å¯èƒ½ã§ã™ã€‚",
            section="ç¬¬5ç«  è‚²å…ãƒ»ä»‹è­·ä¼‘æ¥­",
            file_path="./data/pdfs/å°±æ¥­è¦å‰‡.pdf"
        ),
        SearchHit(
            file_name="è‚²å…ä»‹è­·ä¼‘æ¥­è¦ç¨‹.pdf",
            page_no=3,
            score=80.0,
            text="è‚²å…ä¼‘æ¥­ã®ç”³è«‹ã¯ã€ä¼‘æ¥­é–‹å§‹äºˆå®šæ—¥ã®1ã‹æœˆå‰ã¾ã§ã«ã€æ‰€å®šã®ç”³è«‹æ›¸ã‚’äººäº‹éƒ¨ã«æå‡ºã—ã¦ãã ã•ã„ã€‚",
            section="ç¬¬2æ¡ ç”³è«‹æ‰‹ç¶šã",
            file_path="./data/pdfs/è‚²å…ä»‹è­·ä¼‘æ¥­è¦ç¨‹.pdf"
        )
    ]

    # LLMãŒç„¡åŠ¹ãªè¨­å®šã§ãƒ†ã‚¹ãƒˆ
    with patch.dict(os.environ, {"LLM_PROVIDER": "none"}):
        # ç°¡æ˜“çš„ãªå›ç­”ã‚’ç”Ÿæˆï¼ˆLLMãªã—ï¼‰
        answer = generate_llm_answer(
            query="è‚²å…ä¼‘æ¥­ã¯ã„ã¤ã¾ã§å–ã‚Œã¾ã™ã‹ï¼Ÿ",
            search_results=mock_hits,
            context=None,
            use_llm=False
        )

        print(f"Query: è‚²å…ä¼‘æ¥­ã¯ã„ã¤ã¾ã§å–ã‚Œã¾ã™ã‹ï¼Ÿ")
        print(f"Answer (without LLM):\n{answer}\n")

        assert "è‚²å…ä¼‘æ¥­" in answer or "1æ­³" in answer
        print("âœ“ Fallback answer generation works")


def test_llm_answer_generator_with_mock_api():
    """ãƒ¢ãƒƒã‚¯APIã‚’ä½¿ç”¨ã—ã¦LLMç”Ÿæˆå™¨ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("\n=== Test LLM Answer Generator With Mock API ===")

    # ãƒ¢ãƒƒã‚¯ã®æ¤œç´¢çµæœ
    mock_hits = [
        SearchHit(
            file_name="å°±æ¥­è¦å‰‡.pdf",
            page_no=10,
            score=95.0,
            text="æœ‰çµ¦ä¼‘æš‡ã¯å…¥ç¤¾6ã‹æœˆå¾Œã‹ã‚‰10æ—¥ä»˜ä¸ã•ã‚Œã¾ã™ã€‚",
            section="ç¬¬4ç«  ä¼‘æš‡",
            file_path="./data/pdfs/å°±æ¥­è¦å‰‡.pdf"
        )
    ]

    # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ãƒ¢ãƒƒã‚¯
    with patch('pdf.llm_answer.OpenAI') as MockOpenAI:
        # ãƒ¢ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¨­å®š
        mock_client = MagicMock()
        mock_response = MagicMock()
        mock_response.choices = [
            MagicMock(message=MagicMock(content="æœ‰çµ¦ä¼‘æš‡ã¯å…¥ç¤¾6ã‹æœˆå¾Œã‹ã‚‰10æ—¥ä»˜ä¸ã•ã‚Œã¾ã™ã€‚ãã®å¾Œã€å‹¤ç¶šå¹´æ•°ã«å¿œã˜ã¦æ—¥æ•°ãŒå¢—åŠ ã—ã¾ã™ã€‚\n\nå‡ºå…¸: å°±æ¥­è¦å‰‡.pdf - ãƒšãƒ¼ã‚¸10"))
        ]
        mock_client.chat.completions.create.return_value = mock_response
        MockOpenAI.return_value = mock_client

        # LLMè¨­å®š
        config = LLMConfig(
            provider="openai",
            api_key="mock-api-key",
            model="gpt-4o-mini",
            temperature=0.3
        )

        # ç”Ÿæˆå™¨ã‚’åˆæœŸåŒ–
        generator = LLMAnswerGenerator(config)

        # å›ç­”ã‚’ç”Ÿæˆ
        answer = generator.generate_answer(
            query="æœ‰çµ¦ä¼‘æš‡ã¯ã„ã¤ã‹ã‚‰å–ã‚Œã¾ã™ã‹ï¼Ÿ",
            search_results=mock_hits,
            context=None
        )

        print(f"Query: æœ‰çµ¦ä¼‘æš‡ã¯ã„ã¤ã‹ã‚‰å–ã‚Œã¾ã™ã‹ï¼Ÿ")
        print(f"Answer (with mock LLM):\n{answer}\n")

        assert "å…¥ç¤¾6ã‹æœˆ" in answer or "10æ—¥" in answer
        print("âœ“ LLM answer generation with mock API works")


def test_search_with_llm_integration():
    """æ¤œç´¢ã¨LLMçµ±åˆã®ãƒ†ã‚¹ãƒˆ"""
    print("\n=== Test Search With LLM Integration ===")

    # ãƒ¢ãƒƒã‚¯æ¤œç´¢çµæœ
    mock_hits = [
        SearchHit(
            file_name="çµ¦ä¸è¦ç¨‹.pdf",
            page_no=5,
            score=100.0,
            text="çµ¦ä¸ã®æ”¯æ‰•æ—¥ã¯æ¯æœˆ25æ—¥ã¨ã™ã‚‹ã€‚ãŸã ã—ã€25æ—¥ãŒä¼‘æ—¥ã®å ´åˆã¯å‰å–¶æ¥­æ—¥ã«æ”¯æ‰•ã†ã€‚",
            section="ç¬¬3æ¡ æ”¯æ‰•æ—¥",
            file_path="./data/pdfs/çµ¦ä¸è¦ç¨‹.pdf"
        )
    ]

    # searché–¢æ•°ã‚’ãƒ¢ãƒƒã‚¯
    with patch('pdf.search_with_llm.search') as mock_search:
        mock_search.return_value = mock_hits

        # LLMã‚’ç„¡åŠ¹ã«ã—ã¦ãƒ†ã‚¹ãƒˆï¼ˆç’°å¢ƒå¤‰æ•°ã§åˆ¶å¾¡ï¼‰
        with patch.dict(os.environ, {"LLM_PROVIDER": "none"}):
            result = search_with_llm(
                query="çµ¦ä¸ã®æ”¯æ‰•æ—¥ã¯ã„ã¤ã§ã™ã‹ï¼Ÿ",
                top_k=5,
                use_llm=False
            )

            print(f"Query: {result.query}")
            print(f"Answer:\n{result.answer}\n")
            print(f"Confidence: {result.confidence}")
            print(f"LLM Used: {result.llm_used}")
            print(f"Sources: {len(result.sources)} sources found")

            assert isinstance(result, SearchWithLLMResult)
            assert result.query == "çµ¦ä¸ã®æ”¯æ‰•æ—¥ã¯ã„ã¤ã§ã™ã‹ï¼Ÿ"
            assert result.confidence == "high"  # ã‚¹ã‚³ã‚¢100ãªã®ã§high
            assert not result.llm_used  # LLMã¯ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„
            assert len(result.sources) == 1

            print("âœ“ Search with LLM integration works")


def test_format_search_result():
    """æ¤œç´¢çµæœã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒ†ã‚¹ãƒˆ"""
    print("\n=== Test Format Search Result ===")

    # ãƒ†ã‚¹ãƒˆç”¨ã®çµæœã‚’ä½œæˆ
    test_result = SearchWithLLMResult(
        query="ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª",
        answer="ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆå›ç­”ã§ã™ã€‚",
        search_hits=[
            SearchHit(
                file_name="test.pdf",
                page_no=1,
                score=90.0,
                text="ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆ",
                section="ãƒ†ã‚¹ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³",
                file_path="./test.pdf"
            )
        ],
        snippet="ãƒ†ã‚¹ãƒˆã‚¹ãƒ‹ãƒšãƒƒãƒˆ",
        confidence="high",
        sources=[
            {
                "file_name": "test.pdf",
                "page_no": 1,
                "score": 90.0,
                "section": "ãƒ†ã‚¹ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³",
                "preview": "ãƒ†ã‚¹ãƒˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼"
            }
        ],
        llm_used=True
    )

    # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    formatted = format_search_result(test_result)

    print(f"Formatted result:\n{formatted}\n")

    assert "**å›ç­”:**" in formatted
    assert "ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆå›ç­”ã§ã™" in formatted
    assert "ğŸŸ¢" in formatted  # high confidence
    assert "ğŸ¤–" in formatted  # LLM used
    assert "test.pdf" in formatted

    print("âœ“ Result formatting works")


def test_llm_error_handling():
    """LLMã‚¨ãƒ©ãƒ¼å‡¦ç†ã®ãƒ†ã‚¹ãƒˆ"""
    print("\n=== Test LLM Error Handling ===")

    mock_hits = [
        SearchHit(
            file_name="test.pdf",
            page_no=1,
            score=90.0,
            text="ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆ",
            section="ãƒ†ã‚¹ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³",
            file_path="./test.pdf"
        )
    ]

    # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ãƒ¢ãƒƒã‚¯ã—ã¦ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿã•ã›ã‚‹
    with patch('pdf.llm_answer.OpenAI') as MockOpenAI:
        mock_client = MagicMock()
        mock_client.chat.completions.create.side_effect = Exception("API Error")
        MockOpenAI.return_value = mock_client

        config = LLMConfig(
            provider="openai",
            api_key="mock-api-key"
        )

        generator = LLMAnswerGenerator(config)

        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§å›ç­”ãŒç”Ÿæˆã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
        answer = generator.generate_answer(
            query="ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª",
            search_results=mock_hits,
            context=None
        )

        print(f"Answer after error (fallback):\n{answer}\n")

        assert answer  # ä½•ã‹å›ç­”ãŒè¿”ã•ã‚Œã‚‹
        assert "test.pdf" in answer or "ãƒ†ã‚¹ãƒˆ" in answer

        print("âœ“ Error handling with fallback works")


def main():
    """å…¨ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
    print("\n" + "=" * 60)
    print("LLM Integration Tests")
    print("=" * 60)

    try:
        test_llm_answer_generator_without_api()
        test_llm_answer_generator_with_mock_api()
        test_search_with_llm_integration()
        test_format_search_result()
        test_llm_error_handling()

        print("\n" + "=" * 60)
        print("âœ… All tests passed!")
        print("=" * 60)

    except AssertionError as e:
        print(f"\nâŒ Test failed: {e}")
        raise
    except Exception as e:
        print(f"\nâŒ Unexpected error: {e}")
        raise


if __name__ == "__main__":
    main()